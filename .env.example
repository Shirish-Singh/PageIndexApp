# PageIndex Document QA - Environment Configuration
# Copy this file to .env and fill in your API keys

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Choose your LLM provider: groq, openai, or ollama
LLM_PROVIDER=groq

# Optional: Override the default model for your provider
# LLM_MODEL=llama-3.3-70b-versatile

# Optional: Adjust generation parameters
# LLM_TEMPERATURE=0.7
# LLM_MAX_TOKENS=8192

# =============================================================================
# API Keys (provide the key for your chosen provider)
# =============================================================================

# Groq API Key (get free key at https://console.groq.com)
GROQ_API_KEY=your_groq_api_key_here

# OpenAI API Key (if using OpenAI provider)
# OPENAI_API_KEY=your_openai_api_key_here
# Or use the legacy name:
# CHATGPT_API_KEY=your_openai_api_key_here

# =============================================================================
# Ollama Configuration (for local models)
# =============================================================================

# Ollama base URL (default: http://localhost:11434/v1)
# OLLAMA_BASE_URL=http://localhost:11434/v1

# =============================================================================
# Available Models by Provider
# =============================================================================
#
# Groq (free tier available):
#   - llama-3.3-70b-versatile (recommended)
#   - llama-3.1-70b-versatile
#   - llama-3.1-8b-instant (faster, smaller)
#   - mixtral-8x7b-32768
#   - gemma2-9b-it
#
# OpenAI:
#   - gpt-4o-2024-11-20 (recommended)
#   - gpt-4o-mini (cheaper)
#   - gpt-4-turbo
#   - gpt-3.5-turbo
#
# Ollama (local):
#   - llama3.2
#   - llama3.1
#   - mistral
#   - mixtral
#   - phi3
#   - gemma2
